decision_tree:
  criterion:
    - squared_error
    - friedman_mse
    - absolute_error
    - poisson
  # 'splitter':['best','random'],
  # 'max_features':['sqrt','log2'],

random_forest:
  n_estimators:
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256
  # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
  # 'max_features':['sqrt','log2',None],

gradient_boosting:
  learning_rate:
    - 0.1
    - 0.01
    - 0.05
    - 0.001
  subsample:
    - 0.6
    - 0.7
    - 0.75
    - 0.8
    - 0.85
    - 0.9
  n_estimators:
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256
  # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],
  # 'criterion':['squared_error', 'friedman_mse'],
  # 'max_features':['auto','sqrt','log2'],

linear_regression: {}

xGBRegressor:
  learning_rate:
    - 0.1
    - 0.01
    - 0.05
    - 0.001
  n_estimators:
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256

# catBoosting_regressor:
  # depth:
    # - 6
    # - 8
    # - 10
  # learning_rate:
    # - 0.01
    # - 0.05
    # - 0.1
  # iterations:
    # - 30
    # - 50
    # - 100

# adaBoost_regressor:
  # learning_rate:
    # - 0.1
    # - 0.01
    # - 0.5
    # - 0.001
  # n_estimators:
    # - 8
    # - 16
    # - 32
    # - 64
    # - 128
    # - 256
  # 'loss':['linear','square','exponential']
